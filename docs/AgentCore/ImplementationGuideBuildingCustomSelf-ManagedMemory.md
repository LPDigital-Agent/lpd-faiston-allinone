### Implementation Guide: Building a Custom Self-Managed Memory Pipeline with AgentCore

Tired of building agents that forget users from one sentence to the next? AgentCore Memory solves this by providing a managed persistence layer for your AI agents, enabling both short-term context and long-term knowledge retention. In this guide, we'll go beyond the basics and show you how to build a fully custom, self-managed memory pipeline for complete control over how your agent learns and remembers.The difference in user experience between an agent with and without memory is profound. An agent without memory treats every interaction as its first, lacking awareness of previous exchanges, while a memory-enabled agent builds on past context to provide richer, more relevant responses.| Agents without memory | Agents with memory enabled || \------ | \------ ||  |  || **Description:**  The agent handles each interaction independently, without carrying over context from previous sessions. This forces the user to repeat information and leads to a disconnected, often frustrating experience. The agent cannot learn from past conversations, limiting its ability to personalize its assistance. | **Description:**  With context carried forward, the agent delivers smarter, more natural, and personalized interactions every time. It can reference previous facts, recall user preferences, and understand the broader goal of a conversation, leading to a more efficient and satisfying user journey. |  
AgentCore offers three distinct strategies for processing conversational data from short-term memory into structured long-term knowledge:

* **Built-in:**  This strategy utilizes a fully managed AgentCore pipeline for memory extraction and consolidation. It provides pre-configured logic for common use cases like identifying facts, user preferences, and creating summaries, offering a powerful, zero-maintenance solution.  
* **Built-in with overrides:**  This approach extends the built-in strategies by allowing targeted customization. You can modify the prompts used in the extraction and consolidation steps and select a specific Amazon Bedrock foundation model, all while still leveraging the managed AgentCore pipeline for execution.  
* **Self-managed:**  This strategy grants you complete ownership over the memory processing pipeline. You are responsible for building and operating the custom extraction and consolidation algorithms, giving you maximum control over how your agent's memory is formed.Choosing the  **Self-managed**  strategy is ideal for advanced or highly specialized use cases where default behaviors are insufficient. Its key differentiators center on providing ultimate control and flexibility.  
* **Complete Algorithmic Control:**  You define the precise logic for what constitutes a "memory." This is essential for domain-specific applications where the value of information is nuanced, such as identifying proprietary product details in a sales conversation or specific technical parameters in a support session.  
* **Custom Consolidation Rules:**  You have full authority over how new information interacts with existing memories. This allows you to implement sophisticated deduplication, merging, and updating logic that aligns perfectly with your business rules, preventing data redundancy and ensuring memory accuracy over time.  
* **Proprietary Data Processing:**  If your memory extraction requires proprietary models or data processing techniques that cannot be run within the AgentCore-managed environment, a self-managed pipeline is the only viable approach.  
* **End-to-End Ownership:**  While this strategy requires more development effort, it provides complete control over the entire data flow, from notification handling to final ingestion. This complete ownership means you are also responsible for key security considerations, such as preventing memory poisoning, which we will detail in the Best Practices section.Now, let's explore the reference architecture for implementing a robust, self-managed memory pipeline.

##### 2.0 Architectural Overview of the Self-Managed Pipeline

A well-defined architecture is the foundation of a successful self-managed memory pipeline. This approach grants maximum flexibility over how your agent learns and remembers, but it demands a clear understanding of the data flow and the responsibilities of each component. By implementing this architecture, you take full control of transforming raw conversational data into a curated, long-term knowledge base for your agent.The self-managed pipeline follows four primary stages, initiated by a notification from AgentCore Memory:

1. **Notification Handling:**  The workflow begins when your custom logic receives a job notification from an Amazon SNS topic. This notification contains a reference to a payload object in an Amazon S3 bucket. Your logic is responsible for parsing this message, retrieving the S3 object location, and downloading the batched event payload.  
2. **Memory Extraction:**  With the raw conversational events from the payload, your custom logic applies its unique algorithms to extract relevant information. This is where you can use foundation models or other techniques to identify key facts, user preferences, or other insights you wish to persist.  
3. **Memory Consolidation:**  After extracting new potential memories, your logic must intelligently deduplicate and merge this information with existing records. This critical step prevents redundancy and ensures the long-term memory store remains coherent and accurate.  
4. **Batch Ingestion:**  Finally, your custom logic calls the AgentCore Memory data plane APIs to store the processed and consolidated memories. This completes the loop, making the new knowledge available for the agent in future interactions.This architecture relies on several key AWS services working in concert:  
* **Amazon S3:**  Serves as the delivery location where AgentCore Memory deposits batched event payloads. Your custom pipeline reads raw conversational data from this bucket.  
* **Amazon SNS:**  Acts as the notification service. AgentCore Memory publishes a message to an SNS topic to signal that a new batch of events is ready for processing, which in turn triggers your custom logic.  
* **Custom Processing Logic:**  This is the core, developer-owned component of the pipeline. This component, often implemented as an AWS Lambda function, can leverage agentic frameworks like Strands or CrewAI and call foundation models from Amazon Bedrock to execute its sophisticated extraction and consolidation tasks.  
* **AgentCore Memory API:**  This is the data plane endpoint for AgentCore Memory. Your custom logic uses these APIs to write the final, processed long-term memories back into a specified namespace for your agent to use.The next section details the initial infrastructure setup required to support this architecture.

##### 3.0 Step 1: Foundational Infrastructure Setup

Before implementing the processing logic, you must establish a robust infrastructure to handle the data flow from AgentCore Memory. This involves creating and configuring the necessary AWS resources to receive event payloads and notifications securely and reliably.Follow these steps to configure the foundational components of your pipeline:

* **Create an S3 Bucket:**  Provision a new Amazon S3 bucket in your AWS account. This bucket is the initial handoff point where AgentCore delivers the raw material—the conversational data—for your pipeline to process.  
* **Best Practice:**  Configure a lifecycle policy to automatically delete objects after processing to control costs and maintain a clean data environment.  
* **Create an SNS Topic:**  Set up an Amazon SNS topic that will receive job notifications from AgentCore Memory. This topic acts as the starting gun for your workflow, sending a notification that triggers your custom logic the moment new data is available.  
* **Note:**  Use FIFO topics if processing order within sessions is important for your use case. This ensures that event batches are processed sequentially, which can be critical for maintaining conversational context.  
* **Configure IAM Permissions:**  Create an AWS Identity and Access Management (IAM) role with a trust policy that allows the AgentCore Memory service to assume it. Attach permissions to this role that grant AgentCore Memory the specific s3:PutObject and sns:Publish permissions required to deliver payloads to your S3 bucket and send notifications to your SNS topic. As a best practice, check for relevant AWS managed policies that may serve as a starting point before creating custom policies from scratch.With this infrastructure in place, you are ready to build the custom logic that will consume and process the data.

##### 4.0 Step 2: Implementing Notification and Payload Handling

This step represents the entry point for your custom processing logic. The component you build here acts as a listener, initiating the memory extraction workflow as soon as it receives a notification that new conversational data is available. This is the trigger that kicks off the entire self-managed pipeline.To handle the incoming data, your custom logic must perform the following sequence of actions:

1. **Consume SNS Notification:**  Configure your custom logic (e.g., an AWS Lambda function) as a subscriber to the SNS topic you created in the previous step. This ensures it is automatically invoked whenever AgentCore Memory publishes a new job notification.  
2. **Extract S3 Object Information:**  Upon invocation, your logic must parse the incoming SNS message. The message body will contain a JSON object with details about the event payload, including the S3 bucket name and the object key needed to locate the file.  
3. **Download and Prepare Payload:**  Using the extracted bucket name and object key, implement code to download the payload object from S3. This object contains the batched conversational events from short-term memory, which can now be parsed and prepared for the next stage of processing.Once the event payload is successfully downloaded and parsed, the core tasks of extraction and consolidation can begin.

##### 5.0 Step 3: Designing Custom Memory Extraction Logic

The strategic goal of memory extraction is to intelligently identify and structure useful insights from the raw conversational events stored in short-term memory. This process transforms unstructured dialogue into structured, long-term knowledge that your agent can leverage in future interactions.While you are building custom logic, its objective is similar to AgentCore's built-in strategies. Your custom logic can be designed to extract various types of information to suit your agent's specific needs. Consider designing your logic to identify:

* **Factual Information (Semantic Extraction):**  Identify key facts, entities, and relationships. For our support agent, the custom extraction logic would be designed to specifically identify and pull out orderId and supportCaseNumber from the user's messages.  
* **User Preferences:**  Capture explicitly stated preferences ("I prefer non-dairy options") or implicitly inferred ones from conversational patterns.  
* **Summaries:**  Generate concise summaries of an entire conversation or a specific session.  
* **Episodic Insights:**  Analyze sequences of events to derive insights about the assistant's reasoning, actions, and overall effectiveness in a given conversation.**Pro-Tip:**  Version your extraction prompts. As you refine what information your agent should remember, keeping your prompts in source control will be critical for maintaining and debugging your agent's memory over time.To handle this complex task, you should use a foundation model from a service like  **Amazon Bedrock** . By engineering a precise prompt, you can instruct the model to act as a powerful and flexible extraction engine for your pipeline.After extracting new memories from the conversation, the next logical step is to consolidate them with existing knowledge.

##### 6.0 Step 4: Building Custom Memory Consolidation Logic

Memory consolidation is where your agent's intelligence is refined. This critical decision-making process prevents data redundancy and ensures your agent's long-term memory remains coherent, accurate, and useful. Poor consolidation leads to a cluttered, unreliable memory that degrades agent performance. Effective consolidation logic is the key to building a high-quality, trustworthy knowledge base that improves over time.Your custom consolidation logic should be designed to evaluate each newly extracted fact against the existing long-term memory and choose one of the following core operations:| Operation | Decision Guideline || \------ | \------ || **AddMemory** | Create a new memory record when an extracted fact introduces entirely new information not covered by existing memories. || **UpdateMemory** | Merge new details with an existing memory record, preserving and extending the information coherently. For our support agent, if a new message contains the same orderId but a new supportCaseNumber, your UpdateMemory logic would add the new case number to the existing memory record for that order. || **SkipMemory** | Discard the extracted fact if it is redundant, irrelevant, or does not add value to the existing memory. |  
Effective consolidation is the key to building a reliable and high-quality knowledge base for your agent. Without it, the long-term memory store can quickly become cluttered with duplicate or conflicting information, degrading the agent's performance and leading to inconsistent responses.With the processing logic now complete, the final step is to ingest the curated results back into AgentCore.

##### 7.0 Step 5: Ingesting Processed Memories into AgentCore

The final stage of the self-managed pipeline involves persisting the processed and consolidated memories into AgentCore's long-term storage. This makes the newly acquired knowledge accessible to your agent for all future conversations, completing the cycle of learning and recall.This is accomplished by using the AgentCore Memory data plane APIs.Your custom logic must call the relevant data plane APIs to persist the consolidated memories. This final action, referred to conceptually as "batch ingestion," will execute operations like AddMemory or UpdateMemory within a specified namespace to write the structured insights generated by your custom pipeline back into the managed memory service.With this final step, the custom memory processing loop is complete. You have successfully transformed raw conversational data into structured, durable, long-term knowledge that will empower your agent to deliver more intelligent and personalized experiences.

##### 8.0 Security and Best Practices

A custom, self-managed pipeline offers unparalleled control, but it also requires diligent attention to security and operational best practices to ensure reliability, data integrity, and protection against potential threats.Adhering to the following security considerations is crucial for building a robust and secure solution:

* **Memory Poisoning and Prompt Injection:**  These risks occur when malicious input is saved in memory or used to manipulate the extraction logic. Under the AWS shared responsibility model, you are responsible for input validation and preventing prompt injection vulnerabilities in your custom extraction service. Sanitize user input before writing it to short-term memory and design your extraction prompts to be resilient against manipulation.  
* **Data Encryption:**  Your data in AgentCore Memory is always encrypted at rest. By default, an AWS-owned KMS key is used. For additional control and to meet specific compliance requirements, you can choose to encrypt your memory with a customer-managed KMS key from your own AWS account.  
* **Least-Privilege Principle:**  Create fine-grained, identity-based policies for all components in your pipeline. The IAM role used by your custom processing logic (e.g., AWS Lambda) should have only the minimum permissions necessary to read from S3, write to AgentCore Memory, and interact with any other required services like Amazon Bedrock. This minimizes the potential impact of a security breach.By building a self-managed pipeline, you've unlocked the highest level of control over your agent's cognitive architecture. This powerful pattern enables you to create truly differentiated, domain-aware AI assistants. We're excited to see what you build.

